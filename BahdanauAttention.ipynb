{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPM4akG7T5vUHBxojFAqeiw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sutharimanikanta/Golab/blob/main/BahdanauAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load the dataset\n",
        "data = pd.read_excel(\"/content/engtotel.xlsx\")\n",
        "\n",
        "# Split the dataset into English and Telugu sentences\n",
        "english_sentences = data[\"english\"].values\n",
        "telugu_sentences = data[\"telugu\"].values\n",
        "# # Assume X_english is ready with English sequences, similarly prepare X_telugu\n",
        "# english_sentences = data[\"english\"].values\n",
        "# telugu_sentences = data[\"telugu\"].values\n"
      ],
      "metadata": {
        "id": "a-gDVEkBNwgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mk7n734FTtdu",
        "outputId": "715be277-2210-4fee-842d-6455faac3b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from nltk.probability import FreqDist\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the preprocess function\n",
        "def preprocess(text):\n",
        "    text = str(text)\n",
        "    # Split the text using regex for both English and Telugu\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    # Filter out empty strings\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    # Tokenize the preprocessed text\n",
        "    tokens = nltk.word_tokenize(' '.join(preprocessed))\n",
        "    return tokens\n",
        "\n",
        "# Preprocess the English and Telugu sentences\n",
        "tokens_en = [preprocess(sent) for sent in english_sentences]\n",
        "tokens_te = [preprocess(sent) for sent in telugu_sentences]\n",
        "\n",
        "# Flatten the lists of tokens\n",
        "tokens_en_flat = [token for sublist in tokens_en for token in sublist]\n",
        "tokens_te_flat = [token for sublist in tokens_te for token in sublist]\n",
        "\n",
        "# Create frequency distributions for each language\n",
        "freq_dist_en = FreqDist(tokens_en_flat)\n",
        "freq_dist_te = FreqDist(tokens_te_flat)\n",
        "\n",
        "# Create sorted lists of unique words for each language\n",
        "all_words_en = sorted(list(set(tokens_en_flat)))\n",
        "all_words_te = sorted(list(set(tokens_te_flat)))\n",
        "\n",
        "# Create dictionaries to map tokens to token IDs for each language\n",
        "vocab_en = {token: integer for integer, token in enumerate(all_words_en)}\n",
        "vocab_te = {token: integer for integer, token in enumerate(all_words_te)}\n",
        "\n",
        "# Convert tokens to token IDs for each language\n",
        "token_ids_en = [[vocab_en[token] for token in sent] for sent in tokens_en]\n",
        "token_ids_te = [[vocab_te[token] for token in sent] for sent in tokens_te]\n",
        "\n"
      ],
      "metadata": {
        "id": "WB4zagwyztfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size=1000, embedding_size=128):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embedding_size)\n",
        "        self.gru = tf.keras.layers.GRU(self.embedding_size, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        words = inputs\n",
        "        embeddings = self.embedding_layer(words)\n",
        "        output, state = self.gru(embeddings)\n",
        "        return (output, state)\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, words=20, embedding_size=128):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.words = words\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        self.W1 = self.add_weight(shape=(1, self.embedding_size), initializer=\"random_uniform\")\n",
        "        self.W2 = self.add_weight(shape=(self.words, self.embedding_size), initializer=\"random_uniform\")\n",
        "        self.W3 = self.add_weight(shape=(self.words, self.embedding_size), initializer=\"random_uniform\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, value = inputs\n",
        "        regressed_query = tf.einsum(\"bi,ci -> bi\", query, self.W1)\n",
        "        regressed_value = tf.einsum(\"bij,ij -> bij\", value, self.W2)\n",
        "        sum_query_value = tf.einsum(\"bi,bji -> bji\", regressed_query, regressed_value)\n",
        "        sum_of_query_value = tf.nn.tanh(sum_query_value)\n",
        "        a = tf.einsum(\"bij,ij -> bij\", sum_of_query_value, self.W3)\n",
        "        a = tf.reduce_sum(a, axis=-1)\n",
        "        a = tf.nn.softmax(a)\n",
        "        context = tf.einsum(\"bi,bij -> bij\", a, value)\n",
        "        context = tf.reduce_sum(context, axis=1)\n",
        "        return context\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_size=128, vocab_size=1000, words=20):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.words = words\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        self.attention = BahdanauAttention(words=self.words, embedding_size=self.embedding_size)\n",
        "        self.gru = tf.keras.layers.GRU(self.embedding_size)\n",
        "        self.op1 = tf.keras.layers.Dense(self.embedding_size * 10, activation='tanh')\n",
        "        self.op2 = tf.keras.layers.Dense(self.embedding_size * 10, activation='tanh')\n",
        "        self.op3 = tf.keras.layers.Dense(self.vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        y, state, encode = inputs\n",
        "        context = self.attention((state, encode))\n",
        "        state_expanded = tf.expand_dims(state, axis=1)\n",
        "        context_expanded = tf.expand_dims(context, axis=1)\n",
        "        y_expanded = tf.expand_dims(y, axis=1)\n",
        "        gru1_input = tf.concat([state_expanded, context_expanded], axis=1)\n",
        "        gru1_input2 = tf.concat([gru1_input, y_expanded], axis=1)\n",
        "        new_state = self.gru(gru1_input2)\n",
        "        g_input = tf.concat([tf.concat([y, context], axis=-1), new_state], axis=-1)\n",
        "        g_output = self.op3(self.op2(self.op1(g_input)))\n",
        "        return g_output, new_state\n",
        "\n",
        "class AdditiveAttentionTranslator:\n",
        "    def __init__(self):\n",
        "        self.encoder_input_words = 20\n",
        "        self.vocab_size = 1000\n",
        "        self.embedding_size = 128\n",
        "        self.epochs = 5\n",
        "        self.batch_size = 200\n",
        "        self.optimizer = tf.keras.optimizers.Adam()\n",
        "        self.loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        self.loss_history = []\n",
        "\n",
        "    def get_enc_dec(self):\n",
        "        x_encoder_input = tf.keras.layers.Input(self.encoder_input_words)\n",
        "        encode = Encoder(vocab_size=self.vocab_size, embedding_size=self.embedding_size)(x_encoder_input)\n",
        "        self.encoder = tf.keras.Model(inputs=x_encoder_input, outputs=encode)\n",
        "\n",
        "        x_decoder_input = tf.keras.layers.Input(1)\n",
        "        x_decoder = tf.keras.layers.Embedding(self.vocab_size, self.embedding_size)(x_decoder_input)\n",
        "        x_state_input = tf.keras.layers.Input(self.embedding_size)\n",
        "        x_states_input = tf.keras.layers.Input((self.encoder_input_words, self.embedding_size))\n",
        "\n",
        "        decode = Decoder(embedding_size=self.embedding_size, vocab_size=self.vocab_size, words=self.encoder_input_words)((x_decoder[:,0], x_state_input, x_states_input))\n",
        "        self.decoder = tf.keras.Model(inputs=[x_decoder_input, x_state_input, x_states_input], outputs=decode)\n",
        "        return self.encoder.summary(), self.decoder.summary()\n",
        "\n",
        "    def generate_data_from_tokens(self, tokens_en, tokens_te, decoder_words=10):\n",
        "       X1 = np.array([seq[:self.encoder_input_words] + [0] * (self.encoder_input_words - len(seq)) for seq in tokens_en])\n",
        "       X2 = np.array([seq[:decoder_words] + [0] * (decoder_words - len(seq)) for seq in tokens_te])\n",
        "       max_seq_length = max(len(seq) for seq in tokens_te)\n",
        "       Y = np.zeros((len(tokens_en), max_seq_length - 1, self.vocab_size))\n",
        "       for i, token_ids in enumerate(tokens_te):\n",
        "           for j in range(1, len(token_ids)):\n",
        "               token_id = token_ids[j]\n",
        "               if token_id < self.vocab_size:\n",
        "                   Y[i, j - 1, token_id] = 1\n",
        "       return X1, X2, Y\n",
        "\n",
        "\n",
        "    def train_translator(self, X1, X2, Y):\n",
        "\n",
        "       tf.get_logger().setLevel('ERROR')\n",
        "       optimizer, loss_fn = self.optimizer, self.loss_fn\n",
        "       epochs, batch_size = self.epochs, self.batch_size\n",
        "       self.loss_history = []\n",
        "       for epoch in range(epochs):\n",
        "           batch_loss = tf.constant(0.0)\n",
        "           total_instances = len(Y)\n",
        "           for batch in tqdm(range(0, total_instances, batch_size)):\n",
        "               with tf.GradientTape() as tape:\n",
        "                   loss_count = tf.constant(0.0)\n",
        "                   x1_train = X1[batch : batch + batch_size]\n",
        "                   x2_train = X2[batch : batch + batch_size]\n",
        "                   y_train = Y[batch : batch + batch_size]\n",
        "                   H, state = self.encoder(x1_train)\n",
        "                   for query_number in range(min(y_train.shape[1], y_train.shape[1])):  # Ensure query_number does not exceed decoder input size\n",
        "                       output, state = self.decoder((tf.expand_dims(x2_train[:, 0], axis=-1), state, H))\n",
        "                       loss_count = loss_count + loss_fn(y_train[:, query_number, :], output)  # Fix indexing for y_train\n",
        "               grads = tape.gradient(loss_count, self.encoder.trainable_weights + self.decoder.trainable_weights)\n",
        "               optimizer.apply_gradients(zip(grads, self.encoder.trainable_weights + self.decoder.trainable_weights))\n",
        "               batch_loss = batch_loss + loss_count\n",
        "           print(\"Epoch: \" + str(epoch + 1) + \"/\" + str(epochs) + \" : Error \" + str(batch_loss.numpy()))\n",
        "           self.loss_history.append(batch_loss.numpy())\n",
        "\n",
        "    def translate_sentence(self, keys, query_start, query_size=None):\n",
        "        if query_size is None:\n",
        "            query_size = self.decoder.input_shape[0][1]\n",
        "        keys = tf.concat([keys, query_start, state, H], axis=0)\n",
        "        H, state = self.encoder(keys)\n",
        "        value = []\n",
        "        state_steps = []\n",
        "        value.append(int(query_start[0][0]))\n",
        "        for query_number in range(query_size):\n",
        "            output, state = self.decoder((query_start, state, H))\n",
        "            query_start = np.argmax(output.numpy(), axis=-1)\n",
        "            value.append(query_start[0])\n",
        "            state_steps.append(state)\n",
        "        return value, state_steps\n",
        "\n",
        "translator = AdditiveAttentionTranslator()\n",
        "translator.get_enc_dec()\n"
      ],
      "metadata": {
        "id": "3Pr2JrY-zwul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HI"
      ],
      "metadata": {
        "id": "bIBNMx7qL1yS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Translate a sentence\n",
        "keys = token_ids_en[0]  # Assuming the first sentence is to be translated\n",
        "query_start = token_ids_te[0]  # Assuming the first Telugu sentence is the target\n",
        "translated_sentence, state_steps = translator.translate_sentence(keys, query_start)\n",
        "\n",
        "# Print the translated sentence\n",
        "print(\"Translated sentence:\", translated_sentence)\n"
      ],
      "metadata": {
        "id": "_by10Qr4LLqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator.save_model('translater.yaml')\n"
      ],
      "metadata": {
        "id": "5XuNpI8oytnK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}