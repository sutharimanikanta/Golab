{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNjz7SCvhLospSBcouPqHBV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sutharimanikanta/Golab/blob/main/Untitled12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SartajBhuvaji/Brain-Tumor-Classification-DataSet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cBFOEKszDo9",
        "outputId": "09318064-a522-406b-ea16-c89d52bba3f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Brain-Tumor-Classification-DataSet'...\n",
            "remote: Enumerating objects: 3039, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 3039 (delta 0), reused 0 (delta 0), pack-reused 3035\u001b[K\n",
            "Receiving objects: 100% (3039/3039), 79.25 MiB | 27.69 MiB/s, done.\n",
            "Updating files: 100% (3264/3264), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jduneqbfzDls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "5I34ibNSyzlp",
        "outputId": "c17d14b8-1825-4d88-af5d-fb468cf77186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of images: 8610\n",
            "Number of training images: 7749\n",
            "Number of testing images: 861\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1938/1938 [00:00<00:00, 10458.39it/s]\n",
            "100%|██████████| 1937/1937 [00:00<00:00, 11435.88it/s]\n",
            "100%|██████████| 1937/1937 [00:00<00:00, 11219.92it/s]\n",
            "100%|██████████| 1937/1937 [00:00<00:00, 11206.78it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (7749, 2) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-49658b40ee76>\u001b[0m in \u001b[0;36m<cell line: 150>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0mcreate_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_data.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[1;32m    547\u001b[0m                            pickle_kwargs=dict(fix_imports=fix_imports))\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (7749, 2) + inhomogeneous part."
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
        "import pickle\n",
        "import time\n",
        "import numpy as np\n",
        "import keras.optimizers\n",
        "from sklearn.metrics import classification_report\n",
        "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0,\n",
        "                          write_graph=True, write_images=False)\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import imutils\n",
        "import cv2\n",
        "import PIL.Image\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "demo_datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05,\n",
        "    rescale=1./255,\n",
        "    shear_range=0.05,\n",
        "    brightness_range=[0.1, 1.5],\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "import os\n",
        "\n",
        "# Define categories\n",
        "categories = [\"glioma_tumor\", \"meningioma_tumor\", \"no_tumor\", \"pituitary_tumor\"]\n",
        "\n",
        "# Define paths for training and testing data\n",
        "train_dir = \"./data/train\"\n",
        "test_dir = \"./data/test\"\n",
        "\n",
        "# Function to create directories\n",
        "def make_directories(base_dir):\n",
        "    for category in categories:\n",
        "        os.makedirs(os.path.join(base_dir, category), exist_ok=True)\n",
        "\n",
        "# Create directories for training data\n",
        "make_directories(train_dir)\n",
        "\n",
        "# Create directory for testing data\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "import os\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def cropAndAugmentation():\n",
        "    # Augmentation Code\n",
        "    IMG_SIZE = 80\n",
        "    ADD_PIXELS = 5  # Adjust as needed\n",
        "    dim = (IMG_SIZE, IMG_SIZE)\n",
        "    cwd = os.getcwd()\n",
        "    directory = [\"glioma_tumor\", \"meningioma_tumor\", \"no_tumor\", \"pituitary_tumor\"]\n",
        "    desired_folder = \"Brain-Tumor-Classification-DataSet/Training/\"\n",
        "\n",
        "    # Create copies of the original dataset with different modifications\n",
        "    datasets = []\n",
        "    for i in range(3):\n",
        "        datasets.append([])\n",
        "\n",
        "    for input_folder in directory:\n",
        "        input_folder_path = os.path.join(cwd, desired_folder + input_folder)\n",
        "        if not os.path.exists(input_folder_path):\n",
        "            raise FileNotFoundError(f\"Input folder {input_folder_path} does not exist.\")\n",
        "\n",
        "        for img in os.listdir(input_folder_path):\n",
        "            image_path = os.path.join(input_folder_path, img)\n",
        "            image = cv2.imread(image_path)\n",
        "\n",
        "            # Resize images for the three datasets\n",
        "            resized_image = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)\n",
        "            rotated_image = cv2.rotate(resized_image, cv2.ROTATE_90_CLOCKWISE)\n",
        "            flipped_image = cv2.flip(resized_image, 1)  # Flip along the vertical direction\n",
        "\n",
        "            # Append resized images to datasets\n",
        "            datasets[0].append(resized_image)\n",
        "            datasets[1].append(rotated_image)\n",
        "            datasets[2].append(flipped_image)\n",
        "\n",
        "    # Concatenate the three datasets into one\n",
        "    concatenated_dataset = np.concatenate(datasets, axis=0)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test = train_test_split(concatenated_dataset, test_size=0.1, random_state=42)\n",
        "    # Create an iterator for training data with augmentation\n",
        "    train_iterator = demo_datagen.flow(X_train, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Store training and testing data\n",
        "    data_dir = \"./data\"\n",
        "    train_dir = os.path.join(data_dir, \"train\")\n",
        "    test_dir = os.path.join(data_dir, \"test\")\n",
        "\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    # Store training images\n",
        "    for idx, img_batch in enumerate(train_iterator):\n",
        "        for img in img_batch:\n",
        "            category_folder = directory[idx % len(directory)]\n",
        "            img_path = os.path.join(train_dir, category_folder, f\"image_{idx}.jpg\")\n",
        "            os.makedirs(os.path.join(train_dir, category_folder), exist_ok=True)\n",
        "            cv2.imwrite(img_path, img)\n",
        "            idx += 1  # Increment index for the next image\n",
        "\n",
        "        if idx >= len(X_train):  # Stop when all original training images are processed\n",
        "            break\n",
        "\n",
        "    # Store testing images\n",
        "    for idx, img in enumerate(X_test):\n",
        "        img_path = os.path.join(test_dir, f\"image_{idx}.jpg\")\n",
        "        cv2.imwrite(img_path, img)\n",
        "    # Count total number of images\n",
        "    total_images = len(X_train) + len(X_test)\n",
        "    print(\"Total number of images:\", total_images)\n",
        "    print(\"Number of training images:\", len(X_train))\n",
        "    print(\"Number of testing images:\", len(X_test))\n",
        "\n",
        "# Execute the function\n",
        "cropAndAugmentation()\n",
        "TEST_DIR = '/content/data/test' # test data folder\n",
        "TRAIN_DIR = '/content/data/train' # train data folder\n",
        "CATEGORIES =[\"glioma_tumor\", \"meningioma_tumor\", \"no_tumor\", \"pituitary_tumor\"]\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "training_data = []\n",
        "\n",
        "def create_training_data():\n",
        "    for category in CATEGORIES:\n",
        "        path = os.path.join(TRAIN_DIR, category)  # create path\n",
        "        class_num = CATEGORIES.index(category)  # get the classification\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "            # iterate over each image per category\n",
        "            img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_COLOR)  # convert to array\n",
        "            training_data.append([img_array, class_num])  # add this to our training_data\n",
        "\n",
        "    random.shuffle(training_data)\n",
        "\n",
        "create_training_data()\n",
        "np.save('train_data.npy', training_data)\n",
        "print(len(training_data))\n",
        "print(\"train data\")\n",
        "print()\n",
        "testing_data=[]\n",
        "\n",
        "def create_testing_data():\n",
        "    for img in tqdm(os.listdir(TEST_DIR)):\n",
        "        img_path = os.path.join(TEST_DIR, img)\n",
        "        if os.path.isfile(img_path):\n",
        "            img_array = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "            if img_array is not None:\n",
        "                class_num = 0  # You may need to adjust this if you have multiple classes\n",
        "                testing_data.append([img_array, class_num])\n",
        "            else:\n",
        "                print(f\"Failed to read image: {img_path}\")\n",
        "        else:\n",
        "            print(f\"Not a file: {img_path}\")\n",
        "\n",
        "    random.shuffle(testing_data)\n",
        "    print(\"Total number of images in testing data:\", len(testing_data))\n",
        "\n",
        "create_testing_data()\n",
        "np.save('test_data.npy',testing_data)\n",
        "print(len(testing_data))\n",
        "print(\"test data\")\n",
        "print()\n",
        " #Convert to NumPy array\n",
        "X_train = np.array([i[0] for i in training_data])\n",
        "Y_train = np.array([i[1] for i in training_data])\n",
        "# Convert to NumPy array\n",
        "X_test = np.array([i[0] for i in testing_data])\n",
        "Y_test = np.array([i[1] for i in testing_data])\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import UpSampling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Activation, Dropout\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "class Autoencoder(Model):\n",
        "    def _init_(self, latent_dim, shape):\n",
        "        super(Autoencoder, self)._init()  # Call super().init_()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.shape = shape\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            layers.InputLayer(input_shape=shape),\n",
        "            layers.Conv2D(128, (2, 2), activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Conv2D(128, (2, 2), activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.MaxPooling2D((2, 2)),\n",
        "            layers.Dropout(0.1),\n",
        "            layers.Conv2D(64, (2, 2), activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Conv2D(64, (2, 2), activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.MaxPooling2D((2, 2)),\n",
        "            layers.Dropout(0.1),\n",
        "            layers.Conv2D(32, (2, 2), activation='relu', padding='same'),\n",
        "            layers.MaxPooling2D((2, 2)),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.UpSampling2D((2, 2))  # Adjusted to match the input shape\n",
        "        ])\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            layers.Conv2DTranspose(32, (2, 2), activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.UpSampling2D((2, 2)),  # Adjusted to match the downsampling factor\n",
        "            layers.Dropout(0.1),\n",
        "            layers.Conv2DTranspose(64, (2, 2), activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Conv2DTranspose(64, (2, 2), activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.UpSampling2D((2, 2)),  # Adjusted to match the downsampling factor\n",
        "            layers.Dropout(0.1),\n",
        "            layers.Conv2DTranspose(128, (2, 2), activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Conv2DTranspose(128, (2, 2), activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(0.1),\n",
        "            layers.Conv2DTranspose(shape[-1], (3, 3), activation='sigmoid', padding='same')\n",
        "        ])\n",
        "        self.build((None,) + shape)  # Explicitly build the model\n",
        "\n",
        "    def call(self, x):\n",
        "        encoded = tf.cast(x, tf.float32)  # Cast input data to float32\n",
        "        encoded = self.encoder(encoded)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "# Assuming shape is properly defined as (80, 80, 3)\n",
        "shape = (80, 80, 3)\n",
        "latent_dim = 4\n",
        "autoencoder = Autoencoder(latent_dim, shape)\n",
        "\n",
        "# Compile the model (add loss function and optimizer)\n",
        "autoencoder.compile(loss='mse',\n",
        "                          optimizer=Adam(learning_rate=0.1),\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "# Assuming X_train and X_test are properly defined\n",
        "history=autoencoder.fit(X_train, X_train,\n",
        "                epochs=50,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_test, X_test)\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kPFYfytRzabz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}