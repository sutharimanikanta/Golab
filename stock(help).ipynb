{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPOH0bdfv5k6nsR2Vr1WRB3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgTJsoklYa1J",
        "outputId": "3e7c5b50-2788-4ecd-a0f2-3f0f69508606",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-527b8981f17d>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'price_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mwaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwave_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "def wave_extraction(data, cutoff=0.01):\n",
        "    \"\"\"\n",
        "    Extracts waves from price data using linear regression to fit two lines\n",
        "    :param data: DataFrame with 'High' and 'Low' columns\n",
        "    :param cutoff: loss cutoff to determine the end of a wave\n",
        "    :return: list of waves\n",
        "    \"\"\"\n",
        "    waves = []\n",
        "    current_wave = []\n",
        "    reg_high = LinearRegression()\n",
        "    reg_low = LinearRegression()\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        current_wave.append(data.iloc[i])\n",
        "        if len(current_wave) > 1:\n",
        "            X = np.arange(len(current_wave)).reshape(-1, 1)\n",
        "            y_high = np.array([candle['High'] for candle in current_wave]).reshape(-1, 1)\n",
        "            y_low = np.array([candle['Low'] for candle in current_wave]).reshape(-1, 1)\n",
        "\n",
        "            reg_high.fit(X, y_high)\n",
        "            reg_low.fit(X, y_low)\n",
        "\n",
        "            loss_high = np.mean((reg_high.predict(X) - y_high) ** 2)\n",
        "            loss_low = np.mean((reg_low.predict(X) - y_low) ** 2)\n",
        "\n",
        "            if (loss_high + loss_low) / 2 > cutoff:\n",
        "                waves.append(current_wave[:-1])\n",
        "                current_wave = [current_wave[-1]]\n",
        "\n",
        "    waves.append(current_wave)\n",
        "    return waves\n",
        "\n",
        "# Example usage\n",
        "data = pd.read_csv('price_data.csv')\n",
        "waves = wave_extraction(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(waves):\n",
        "    \"\"\"\n",
        "    Prepare the data by extracting features from the waves\n",
        "    :param waves: list of waves\n",
        "    :return: DataFrame of features\n",
        "    \"\"\"\n",
        "    features = []\n",
        "\n",
        "    for wave in waves:\n",
        "        num_candles = len(wave)\n",
        "        high_last = wave[-1]['High']\n",
        "        low_last = wave[-1]['Low']\n",
        "        avg_high_low = np.mean([(candle['High'] + candle['Low']) / 2 for candle in wave])\n",
        "        slope_upper = (wave[-1]['High'] - wave[0]['High']) / num_candles\n",
        "        slope_lower = (wave[-1]['Low'] - wave[0]['Low']) / num_candles\n",
        "\n",
        "        indicators = {\n",
        "            'MACD': talib.MACD(np.array([candle['Close'] for candle in wave]))[0][-1],\n",
        "            'EMA': talib.EMA(np.array([candle['Close'] for candle in wave]))[-1],\n",
        "            'SMA': talib.SMA(np.array([candle['Close'] for candle in wave]))[-1],\n",
        "            'ADX': talib.ADX(np.array([candle['High'] for candle in wave]), np.array([candle['Low'] for candle in wave]), np.array([candle['Close'] for candle in wave]))[-1],\n",
        "            'RSI': talib.RSI(np.array([candle['Close'] for candle in wave]))[-1],\n",
        "            'Stochastic': talib.STOCH(np.array([candle['High'] for candle in wave]), np.array([candle['Low'] for candle in wave]), np.array([candle['Close'] for candle in wave]))[0][-1],\n",
        "            'WilliamsR': talib.WILLR(np.array([candle['High'] for candle in wave]), np.array([candle['Low'] for candle in wave]), np.array([candle['Close'] for candle in wave]))[-1],\n",
        "            'BollingerBands': talib.BBANDS(np.array([candle['Close'] for candle in wave]))[0][-1],\n",
        "            'KeltnerChannel': talib.KAMA(np.array([candle['Close'] for candle in wave]))[-1]\n",
        "        }\n",
        "\n",
        "        feature_row = [num_candles, high_last, low_last, avg_high_low, slope_upper, slope_lower] + list(indicators.values())\n",
        "        features.append(feature_row)\n",
        "\n",
        "    columns = ['num_candles', 'high_last', 'low_last', 'avg_high_low', 'slope_upper', 'slope_lower',\n",
        "               'MACD', 'EMA', 'SMA', 'ADX', 'RSI', 'Stochastic', 'WilliamsR', 'BollingerBands', 'KeltnerChannel']\n",
        "\n",
        "    return pd.DataFrame(features, columns=columns)\n",
        "\n",
        "# Example usage\n",
        "feature_data = prepare_data(waves)\n"
      ],
      "metadata": {
        "id": "L2fHdSCeIZRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Conv1D, Dense, Dropout, Input, Add, Activation, Concatenate, LayerNormalization, MultiHeadAttention\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.activations import selu\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import talib\n",
        "\n",
        "# Define the Dilated Causal Convolutional Layer with Skip Connections\n",
        "class DilatedCausalConv(Layer):\n",
        "    def __init__(self, filters, kernel_size, dilation_rate):\n",
        "        super(DilatedCausalConv, self).__init__()\n",
        "        self.conv = Conv1D(filters, kernel_size, dilation_rate=dilation_rate, padding='causal', activation=selu)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv(inputs)\n",
        "        return x\n",
        "\n",
        "# Define the Att-DCNN model\n",
        "def build_att_dcnn(input_shape, num_filters, kernel_size, num_dilations, dropout_rates):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    skip_connections = []\n",
        "\n",
        "    for i in range(num_dilations):\n",
        "        x = DilatedCausalConv(num_filters, kernel_size, dilation_rate=2**i)(x)\n",
        "        skip_connections.append(x)\n",
        "        x = Dropout(dropout_rates[i])(x)\n",
        "\n",
        "    x = Add()(skip_connections)\n",
        "    x = Conv1D(100, 1, activation=selu)(x)\n",
        "    x = Dropout(0.5)(x)  # Final dropout before FC layer\n",
        "    outputs = Dense(100, activation='selu')(x)\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "# Define the Attention Mechanism for Event Embeddings\n",
        "def self_attention(x, units):\n",
        "    attention = Dense(units, activation='tanh')(x)\n",
        "    attention = Dense(units, activation='softmax')(attention)\n",
        "    context = attention * x\n",
        "    return tf.reduce_sum(context, axis=1)\n",
        "\n",
        "# Define the Att-biNTN model\n",
        "def build_att_bintn(input_shape, num_heads, head_size):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs\n",
        "\n",
        "    for _ in range(num_heads):\n",
        "        x = self_attention(x, head_size)\n",
        "\n",
        "    x = Dense(100, activation='selu')(x)\n",
        "    return Model(inputs, x)\n",
        "\n",
        "# Combine the models into Att-DiCE\n",
        "def build_att_dice(financial_input_shape, event_input_shape, num_filters, kernel_size, num_dilations, dropout_rates, num_heads, head_size):\n",
        "    financial_inputs = Input(shape=financial_input_shape)\n",
        "    event_inputs = Input(shape=event_input_shape)\n",
        "\n",
        "    att_dcnn = build_att_dcnn(financial_input_shape, num_filters, kernel_size, num_dilations, dropout_rates)(financial_inputs)\n",
        "    att_bintn = build_att_bintn(event_input_shape, num_heads, head_size)(event_inputs)\n",
        "\n",
        "    combined = Concatenate()([att_dcnn, att_bintn])\n",
        "    outputs = Dense(1, activation='sigmoid')(combined)\n",
        "\n",
        "    return Model([financial_inputs, event_inputs], outputs)\n",
        "\n",
        "# Define the parameters\n",
        "financial_input_shape = (40, 10)  # Example shape\n",
        "event_input_shape = (40, 100)  # Example shape\n",
        "num_filters = 64\n",
        "kernel_size = 3\n",
        "num_dilations = 4\n",
        "dropout_rates = [0.2, 0.3, 0.4, 0.5]\n",
        "num_heads = 3\n",
        "head_size = 100\n",
        "\n",
        "# Build the model\n",
        "att_dice_model = build_att_dice(financial_input_shape, event_input_shape, num_filters, kernel_size, num_dilations, dropout_rates, num_heads, head_size)\n",
        "\n",
        "# Compile the model\n",
        "att_dice_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "att_dice_model.summary()\n"
      ],
      "metadata": {
        "id": "37l6IUOsIcib"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}