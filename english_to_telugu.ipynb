{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNDSJxmDsDHw99VUiCrnqzE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sutharimanikanta/Golab/blob/main/english_to_telugu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7qt4VVFk7he",
        "outputId": "defc58ca-7ce3-4a30-d98c-bb661290c715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/the-verdict.txt\",\"r\",encoding=\"utf-8\")as f:#converting text to binary\n",
        "  raw_text=f.read()\n",
        "print(\"total number of character:\",len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text=\"is legs are long.++++$++++అతని కాళ్ళు పొడవుగా ఉన్నాయి.Who taught Tom how to speak French?++++$+++\"\n",
        "result=re.split(r'(\\s)',text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZBdqAVcmXGf",
        "outputId": "3bf70dac-9ac6-4b4c-8ebb-f9a36e83c9d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['is', ' ', 'legs', ' ', 'are', ' ', 'long.++++$++++అతని', ' ', 'కాళ్ళు', ' ', 'పొడవుగా', ' ', 'ఉన్నాయి.Who', ' ', 'taught', ' ', 'Tom', ' ', 'how', ' ', 'to', ' ', 'speak', ' ', 'French?++++$+++']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result2 = re.sub(r'[++++$++++]', ' ', text)\n",
        "print(result2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA6UKAhWm6Hs",
        "outputId": "48aaf2d1-22c8-4dd8-e644-64454c52cd76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is legs are long.         అతని కాళ్ళు పొడవుగా ఉన్నాయి.Who taught Tom how to speak French?        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result=re.split(r'([,.?_!\"()\\']|--|\\s)' ,result2)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiCk8cNCrQiJ",
        "outputId": "c0010a3f-51e0-4211-821f-65172b017526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['is', ' ', 'legs', ' ', 'are', ' ', 'long', '.', '', ' ', '', ' ', '', ' ', '', ' ', '', ' ', '', ' ', '', ' ', '', ' ', '', ' ', 'అతని', ' ', 'కాళ్ళు', ' ', 'పొడవుగా', ' ', 'ఉన్నాయి', '.', 'Who', ' ', 'taught', ' ', 'Tom', ' ', 'how', ' ', 'to', ' ', 'speak', ' ', 'French', '?', '', ' ', '', ' ', '', ' ', '', ' ', '', ' ', '', ' ', '', ' ', '', ' ', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result=[item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUYc4NO_nxvY",
        "outputId": "02563e42-1fbc-4413-a15d-a8eec9453111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['is', 'legs', 'are', 'long', '.', 'అతని', 'కాళ్ళు', 'పొడవుగా', 'ఉన్నాయి', '.', 'Who', 'taught', 'Tom', 'how', 'to', 'speak', 'French', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed1= re.sub(r'[++++$++++]',' ',raw_text )\n",
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)',preprocessed1)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSmLdzfBqInG",
        "outputId": "e53f813a-8b16-44d3-a7ac-50244b158c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['His', 'legs', 'are', 'long', '.', 'అతని', 'కాళ్ళు', 'పొడవుగా', 'ఉన్నాయి', '.', 'Who', 'taught', 'Tom', 'how', 'to', 'speak', 'French', '?', 'టామ్', 'ఫ్రెంచ్', 'మాట్లాడటం', 'ఎలా', 'నేర్పించారు', '?', 'I', 'swim', 'in', 'the', 'sea', 'every']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# converting tokens to token ids\n"
      ],
      "metadata": {
        "id": "kqCPOfwdsg8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed1= re.sub(r'[++++$++++]',' ',raw_text )\n",
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)',preprocessed1)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "all_words = sorted(list(set(preprocessed)))\n",
        "vocab={token:integer for integer,token in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "0VTK-N6XLZlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(list(set(preprocessed)))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB5VcrbAsgcA",
        "outputId": "8732e624-af4b-4126-8778-d967a12048b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab={token:integer for integer,token in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "KuJoa3NLsS2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GizjteAstUeM",
        "outputId": "44d96b2a-6f63-42ee-98bc-7d3fe5e16ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "('&', 2)\n",
            "(\"'\", 3)\n",
            "(',', 4)\n",
            "('--', 5)\n",
            "('-239', 6)\n",
            "('-244', 7)\n",
            "('-5', 8)\n",
            "('-మైన', 9)\n",
            "('.', 10)\n",
            "('0', 11)\n",
            "('00', 12)\n",
            "('000', 13)\n",
            "('1', 14)\n",
            "('10', 15)\n",
            "('10%', 16)\n",
            "('10-minute', 17)\n",
            "('100', 18)\n",
            "('100%', 19)\n",
            "('1000', 20)\n",
            "('100ºC', 21)\n",
            "('103', 22)\n",
            "('1066', 23)\n",
            "('10:00', 24)\n",
            "('10:30', 25)\n",
            "('10th', 26)\n",
            "('11', 27)\n",
            "('110', 28)\n",
            "('111', 29)\n",
            "('119', 30)\n",
            "('12', 31)\n",
            "('120', 32)\n",
            "('123', 33)\n",
            "('125', 34)\n",
            "('126', 35)\n",
            "('1271', 36)\n",
            "('12:45', 37)\n",
            "('12th', 38)\n",
            "('13', 39)\n",
            "('13-year-old', 40)\n",
            "('130', 41)\n",
            "('133', 42)\n",
            "('13th', 43)\n",
            "('14', 44)\n",
            "('145', 45)\n",
            "('14th', 46)\n",
            "('15', 47)\n",
            "('15-year', 48)\n",
            "('150', 49)\n",
            "('1558', 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "  def __init__(self,vocab):\n",
        "    self.str_to_int=vocab\n",
        "    self.int_to_str={i:s for s,i in vocab.items()}\n",
        "  def encode(self,text):\n",
        "    #preprocessed1= re.sub(r'[++++$++++]',' ',text )\n",
        "    #preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)',preprocessed1)\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)',text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids=[self.str_to_int[s] for  s in preprocessed ]\n",
        "    return ids\n",
        "  def decode(self,ids):\n",
        "    text=\" \".join([self.int_to_str[i] for i in ids])\n",
        "    text=re.sub(r'([,.?_!\"()\\']|--|\\s)',r'\\1',text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "y7-dp4_btbaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenzier=SimpleTokenizerV1(vocab)\n"
      ],
      "metadata": {
        "id": "11-nKGyNwag1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "066418ea-5474-407f-96cf-912a4efd709b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'vocab' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c25a28213fa7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenzier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSimpleTokenizerV1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\"His legs are long.++++$++++అతని కాళ్ళు పొడవుగా ఉన్నాయి.\n",
        "Who taught Tom how to speak French?++++$+++\"\"\"\n",
        "ids = tokenzier.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK6Q0N8Ix_Do",
        "outputId": "df5bce3f-8eb0-4b6e-b349-29b624d14a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1417, 9416, 3615, 9611, 10, 16401, 23578, 39651, 20693, 10, 2984, 14284, 2792, 8546, 14593, 13563, 1231, 304]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenzier.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ws9dH7RByIMF",
        "outputId": "739f360f-e406-4ffb-c0da-939a9756460c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" His legs are long . అతని కాళ్ళు పొడవుగా ఉన్నాయి . Who taught Tom how to speak French ?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding special context tokens\n",
        "Some tokenizers use special tokens to help the LLM with additional context\n",
        "\n",
        "Some of these special tokens are\n",
        "\n",
        "[BOS] (beginning of sequence) marks the beginning of text\n",
        "[EOS] (end of sequence) marks where the text ends (this is usually used to concatenate multiple unrelated texts, e.g., two different Wikipedia articles or two different books, and so on)\n",
        "[PAD] (padding) if we train LLMs with a batch size greater than 1 (we may include multiple texts with different lengths; with the padding token we pad the shorter texts to the longest length so that all texts have an equal length)\n",
        "[UNK] to represent works that are not included in the vocabulary\n",
        "\n",
        "Note that GPT-2 does not need any of these tokens mentioned above but only uses an <|endoftext|> token to reduce complexity\n",
        "\n",
        "The <|endoftext|> is analogous to the [EOS] token mentioned above\n",
        "\n",
        "GPT also uses the <|endoftext|> for padding (since we typically use a mask when training on batched inputs, we would not attend padded tokens anyways, so it does not matter what these tokens are)\n",
        "\n",
        "GPT-2 does not use an <UNK> token for out-of-vocabulary words; instead, GPT-2 uses a byte-pair encoding (BPE) tokenizer, which breaks down words into subword units which we will discuss in a later section\n",
        "\n"
      ],
      "metadata": {
        "id": "dIb_PcD8z2fP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"Hello manikanta, do you like tea. Is this-- a test?\"\n",
        "\n",
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "NRb0nJaC05Wp",
        "outputId": "8423e24b-3c9b-4e90-a4cc-0ef682507d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'manikanta'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-901e94ed140d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello manikanta, do you like tea. Is this-- a test?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-619418cc93c6>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([,.?_!\"()\\']|--|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreprocessed1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m  \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-619418cc93c6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([,.?_!\"()\\']|--|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreprocessed1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m  \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'manikanta'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above produces an error because the word \"Hello\" is not contained in the vocabulary\n",
        "To deal with such cases, we can add special tokens like \"<|unk|>\" to the vocabulary to represent unknown words\n",
        "Since we are already extending the vocabulary, let's add another token called \"<|endoftext|>\" which is used in GPT-2 training to denote the end of a text (and it's also used between concatenated text, like if our training datasets consists of multiple articles, books, etc.)"
      ],
      "metadata": {
        "id": "ca2myyiw1Lvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(list(set(preprocessed)))\n",
        "all_tokens = all_words\n",
        "all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "zSTzrscDyT64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "  def __init__(self,vocab):\n",
        "    self.str_to_int=vocab\n",
        "    self.int_to_str={i:s for s,i in vocab.items()}\n",
        "  def encode(self,text):\n",
        "    preprocessed1= re.sub(r'[++++$++++]',' ',text )\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)',preprocessed1)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    preprocessed=[item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
        "    ids=[self.str_to_int[s] for  s in preprocessed ]\n",
        "    return ids\n",
        "  def decode(self,ids):\n",
        "    text=\" \".join([self.int_to_str[i] for i in ids])\n",
        "    text=re.sub(r'([,.?_!\"()\\']|--|\\s)',r'\\1',text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "BSguIe_52aMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text = \"Hello manikanta, do you like tea. Is this-- a test?\"\n",
        "\n",
        "id=tokenizer.encode(text)"
      ],
      "metadata": {
        "id": "DE1gvSiw26GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "wyk6FfCy28Z_",
        "outputId": "27c3b5a1-c1ff-4e1e-b34c-742334acc167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello <|unk|> , do you like tea . Is this -- a test ?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BytePair encoding"
      ],
      "metadata": {
        "id": "SmkCGtEP3U3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-2 used BytePair encoding (BPE) as its tokenizer\n",
        "it allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n",
        "For instance, if GPT-2's vocabulary doesn't have the word \"unfamiliarword,\" it might tokenize it as [\"unfam\", \"iliar\", \"word\"] or some other subword breakdown, depending on its trained BPE merges\n",
        "The original BPE tokenizer can be found here: https://github.com/openai/gpt-2/blob/master/src/encoder.py\n",
        "In this chapter, we are using the BPE tokenizer from OpenAI's open-source tiktoken library, which implements its core algorithms in Rust to improve computational performance\n",
        "I created a notebook in the ./bytepair_encoder that compares these two implementations side-by-side (tiktoken was about 5x faster on the sample text)"
      ],
      "metadata": {
        "id": "_pXPFyGm3noF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROWlYqUy32Wt",
        "outputId": "96b49deb-f5cd-40a2-a0b0-229a7849b718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "print(\"tiktoken version:\",importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFIQxP8U3Uhl",
        "outputId": "99598d26-e23b-41ff-bcad-f278bafd2791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n"
      ],
      "metadata": {
        "id": "0VY0VWLR3LQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello manikanta, do you like tea. Is this-- a test?\"\n",
        "\n",
        "id=tokenizer.encode(text)"
      ],
      "metadata": {
        "id": "sm3dN08t43MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyzIMUaj5KpF",
        "outputId": "a88ed499-cf06-4d00-9e6e-2112157c0ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[9906,\n",
              " 893,\n",
              " 1609,\n",
              " 8424,\n",
              " 11,\n",
              " 656,\n",
              " 499,\n",
              " 1093,\n",
              " 15600,\n",
              " 13,\n",
              " 2209,\n",
              " 420,\n",
              " 313,\n",
              " 264,\n",
              " 1296,\n",
              " 30]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(id)\n",
        "\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfTMyzhw5NRd",
        "outputId": "cfd0ef04-7b13-4747-ceb7-de69fae622d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello manikanta, do you like tea. Is this-- a test?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "integers = tokenizer.encode(\"Akwirw ier\")\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewG0getu5aYk",
        "outputId": "5f3d7474-8655-4ed5-aae5-3f0d0c7f5fa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[32, 29700, 404, 86, 602, 261]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in integers:\n",
        "    print(f\"{i} -> {tokenizer.decode([i])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYA17a0m5t89",
        "outputId": "e6ad53e8-915b-4938-f6b6-0f1f61c73974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32 -> A\n",
            "29700 -> kw\n",
            "404 -> ir\n",
            "86 -> w\n",
            "602 ->  i\n",
            "261 -> er\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNkPiX_-5yFV",
        "outputId": "5f3c073f-67ec-45e9-9952-f3b384742a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akwirw ier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data sampling with a sliding window"
      ],
      "metadata": {
        "id": "bYquf0RX55X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_text=tokenizer.encode(raw_text)\n",
        "enc_sample = enc_text[50:]"
      ],
      "metadata": {
        "id": "RW00OpvX53Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMEalcSz7PMf",
        "outputId": "753092b1-6dbd-4088-fc4a-e1b54f78017a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [32405, 122, 32405, 107]\n",
            "y:      [122, 32405, 107, 32405]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(context, \"---->\", desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPAIdEZZ7Xwl",
        "outputId": "d696b727-030d-4afe-da27-4acfaceaa3bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[32405] ----> 122\n",
            "[32405, 122] ----> 32405\n",
            "[32405, 122, 32405] ----> 107\n",
            "[32405, 122, 32405, 107] ----> 32405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQZdNi9X7zDP",
        "outputId": "07a6b0ef-544c-4f5d-aeea-e18c17c71baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "� ----> �\n",
            "ా ----> �\n",
            "ా� ----> �\n",
            "ాయ ----> �\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(enc_sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rel5hVVY71wj",
        "outputId": "10599c45-e644-4354-f88e-dced659facaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"pytorch version:\",torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cE02SJw8vaJ",
        "outputId": "ad882b4a-3fbb-4c44-8a10-065d741b9029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytorch version: 2.2.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset,DataLoader\n",
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self,txt,tokenizer,max_length,stride):\n",
        "    self.tokenizer=tokenizer\n",
        "    self.input_ids=[]\n",
        "    self.target_ids=[]\n",
        "    token_ids=tokenizer.encode(txt)\n",
        "    for i in range(0,len(token_ids)- max_length,stride):\n",
        "      input_chunk = token_ids[i:i + max_length]\n",
        "      target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.input_ids)\n",
        "  def __getitem__(self, idx):\n",
        "      return self.input_ids[idx], self.target_ids[idx]\n"
      ],
      "metadata": {
        "id": "uwaw1eZK9hqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def  create_dataloader(txt, batch_size=4, max_length=256, stride=128):\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "9GdrRLWq-O57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader(raw_text, batch_size=1, max_length=4, stride=1)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "id": "8VUwUTxbAgM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader(raw_text, batch_size=8, max_length=4, stride=5)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "id": "yOJOZbPZAz9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating token embeddings\n",
        "The data is already almost ready for an LLM\n",
        "But lastly let us embed the tokens in a continuous vector representation using an embedding layer\n",
        "Usually, these embedding layers are part of the LLM itself and are updated (trained) during model training\n"
      ],
      "metadata": {
        "id": "f3LSB9KhC5Ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "qQXlFdu-C0X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding word positions"
      ],
      "metadata": {
        "id": "UpM_rRoXGcuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To create the input embeddings used in an LLM, we simply add the token and the positional embeddings:"
      ],
      "metadata": {
        "id": "XjV3uDWuHJds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader(raw_text, batch_size=8, max_length=max_length, stride=5)\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ],
      "metadata": {
        "id": "M_1zVn-sHnHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "id": "oQIgIBh7F7VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(block_size, output_dim)"
      ],
      "metadata": {
        "id": "dzQE-aOkHZg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "id": "zwxabEGoHewr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "id": "HG3NN7RtHvFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WYJ-gcheMUcJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}